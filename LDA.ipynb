{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_english_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'user_id', 'review_id', 'text',\n",
       "       'business_id', 'stars', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    69302\n",
       "5    59386\n",
       "3    40021\n",
       "2    22956\n",
       "1    22889\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df[df['stars'] == 5.0]\n",
    "df_4 = df[df['stars'] == 4.0]\n",
    "df_3 = df[df['stars'] == 3.0]\n",
    "df_2 = df[df['stars'] == 2.0]\n",
    "df_1 = df[df['stars'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_5.sample(10000)\n",
    "df_4 = df_4.sample(10000)\n",
    "df_3 = df_3.sample(10000)\n",
    "df_2 = df_2.sample(10000)\n",
    "df_1 = df_1.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = pd.concat([df_5,df_4,df_3,df_2,df_1],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    10000\n",
       "4    10000\n",
       "3    10000\n",
       "2    10000\n",
       "1    10000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df_sampled['text']\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokens = [word_tokenize(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.to_csv('final_sampled_english_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "\n",
    "def remove_stop_words(docs):\n",
    "    en_stops = stopwords.words('english')\n",
    "    en_stops.extend(['should','they','this','came','would','could'])\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_word = []  \n",
    "        for word in doc:\n",
    "            if word not in en_stops:\n",
    "                new_word.append(word)\n",
    "        new_docs.append(new_word)\n",
    "            \n",
    "    return new_docs\n",
    "\n",
    "def filterInput(documents):\n",
    "    new_docs = []\n",
    "    for doc in documents:\n",
    "        new_word = []\n",
    "        for word in doc:\n",
    "            new_word.append(word.lower())\n",
    "            for char in word:\n",
    "                if(not char.isalpha()):\n",
    "                    new_word.remove(word.lower())\n",
    "                    break\n",
    "        new_docs.append(new_word)\n",
    "    \n",
    "    return new_docs\n",
    "\n",
    "import re\n",
    "def remove_punctuation(docs):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_words = []  \n",
    "        for word in doc:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        new_docs.append(new_words)\n",
    "            \n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_filtered = filterLen(docs_tokens,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_wo_stopwords = remove_stop_words(docs_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_filt = filterInput(docs_wo_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_wo_punctuation = remove_punctuation(docs_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "# spacy for lemmatization\n",
    "\n",
    "\n",
    "# # Plotting tools\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Corpus\n",
    "texts = docs_wo_punctuation\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(docs_wo_punctuation)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.024*\"breakfast\" + 0.023*\"eggs\" + 0.016*\"brunch\" + 0.016*\"place\" + '\n",
      "  '0.016*\"good\" + 0.015*\"food\" + 0.010*\"like\" + 0.010*\"service\" + '\n",
      "  '0.008*\"bacon\" + 0.007*\"vegetarian\"'),\n",
      " (1,\n",
      "  '0.013*\"good\" + 0.013*\"dessert\" + 0.009*\"great\" + 0.009*\"food\" + '\n",
      "  '0.008*\"restaurant\" + 0.007*\"service\" + 0.007*\"delicious\" + 0.007*\"also\" + '\n",
      "  '0.007*\"menu\" + 0.006*\"ordered\"'),\n",
      " (2,\n",
      "  '0.019*\"place\" + 0.015*\"restaurant\" + 0.014*\"like\" + 0.012*\"toronto\" + '\n",
      "  '0.010*\"this\" + 0.010*\"food\" + 0.007*\"korean\" + 0.006*\"best\" + '\n",
      "  '0.005*\"restaurants\" + 0.005*\"better\"'),\n",
      " (3,\n",
      "  '0.035*\"food\" + 0.020*\"good\" + 0.018*\"salad\" + 0.016*\"service\" + '\n",
      "  '0.012*\"place\" + 0.011*\"restaurant\" + 0.009*\"menu\" + 0.009*\"steak\" + '\n",
      "  '0.008*\"price\" + 0.008*\"like\"'),\n",
      " (4,\n",
      "  '0.014*\"toast\" + 0.010*\"french\" + 0.007*\"place\" + 0.007*\"brunch\" + '\n",
      "  '0.005*\"like\" + 0.004*\"people\" + 0.004*\"even\" + 0.004*\"food\" + 0.004*\"time\" '\n",
      "  '+ 0.004*\"room\"'),\n",
      " (5,\n",
      "  '0.038*\"sushi\" + 0.019*\"fish\" + 0.018*\"rolls\" + 0.016*\"roll\" + 0.014*\"rice\" '\n",
      "  '+ 0.013*\"salmon\" + 0.012*\"fresh\" + 0.011*\"sashimi\" + 0.011*\"good\" + '\n",
      "  '0.009*\"ordered\"'),\n",
      " (6,\n",
      "  '0.046*\"chicken\" + 0.023*\"burger\" + 0.020*\"food\" + 0.016*\"place\" + '\n",
      "  '0.013*\"good\" + 0.010*\"like\" + 0.010*\"wings\" + 0.009*\"fries\" + 0.009*\"rice\" '\n",
      "  '+ 0.007*\"sauce\"'),\n",
      " (7,\n",
      "  '0.029*\"food\" + 0.016*\"order\" + 0.014*\"time\" + 0.012*\"service\" + '\n",
      "  '0.009*\"place\" + 0.008*\"bill\" + 0.008*\"never\" + 0.008*\"restaurant\" + '\n",
      "  '0.007*\"ordered\" + 0.007*\"even\"'),\n",
      " (8,\n",
      "  '0.015*\"coffee\" + 0.012*\"place\" + 0.011*\"like\" + 0.009*\"cream\" + '\n",
      "  '0.009*\"good\" + 0.007*\"milk\" + 0.007*\"they\" + 0.006*\"also\" + 0.006*\"really\" '\n",
      "  '+ 0.006*\"chocolate\"'),\n",
      " (9,\n",
      "  '0.026*\"soup\" + 0.020*\"good\" + 0.018*\"noodles\" + 0.015*\"food\" + 0.015*\"pork\" '\n",
      "  '+ 0.015*\"rice\" + 0.014*\"place\" + 0.013*\"beef\" + 0.012*\"ramen\" + '\n",
      "  '0.011*\"noodle\"'),\n",
      " (10,\n",
      "  '0.047*\"pasta\" + 0.020*\"sauce\" + 0.016*\"italian\" + 0.012*\"ordered\" + '\n",
      "  '0.009*\"dish\" + 0.008*\"seafood\" + 0.008*\"spaghetti\" + 0.008*\"bread\" + '\n",
      "  '0.007*\"back\" + 0.007*\"like\"'),\n",
      " (11,\n",
      "  '0.038*\"food\" + 0.025*\"service\" + 0.025*\"place\" + 0.013*\"good\" + '\n",
      "  '0.012*\"location\" + 0.011*\"staff\" + 0.010*\"restaurant\" + 0.010*\"always\" + '\n",
      "  '0.009*\"time\" + 0.009*\"great\"'),\n",
      " (12,\n",
      "  '0.035*\"sandwich\" + 0.025*\"cheese\" + 0.011*\"bread\" + 0.009*\"meat\" + '\n",
      "  '0.008*\"sandwiches\" + 0.008*\"fries\" + 0.007*\"like\" + 0.007*\"bacon\" + '\n",
      "  '0.007*\"good\" + 0.006*\"salad\"'),\n",
      " (13,\n",
      "  '0.095*\"pizza\" + 0.012*\"sauce\" + 0.011*\"crust\" + 0.009*\"pizzas\" + '\n",
      "  '0.008*\"good\" + 0.008*\"great\" + 0.006*\"like\" + 0.006*\"toppings\" + '\n",
      "  '0.006*\"thin\" + 0.006*\"tomato\"'),\n",
      " (14,\n",
      "  '0.018*\"food\" + 0.018*\"like\" + 0.010*\"even\" + 0.010*\"asked\" + 0.009*\"back\" + '\n",
      "  '0.008*\"said\" + 0.008*\"never\" + 0.007*\"service\" + 0.006*\"ordered\" + '\n",
      "  '0.006*\"really\"'),\n",
      " (15,\n",
      "  '0.033*\"burrito\" + 0.026*\"tacos\" + 0.018*\"taco\" + 0.017*\"chips\" + '\n",
      "  '0.015*\"fish\" + 0.015*\"mexican\" + 0.010*\"bowl\" + 0.008*\"salsa\" + '\n",
      "  '0.008*\"nachos\" + 0.007*\"guacamole\"'),\n",
      " (16,\n",
      "  '0.019*\"chicken\" + 0.018*\"sauce\" + 0.015*\"fries\" + 0.014*\"ordered\" + '\n",
      "  '0.012*\"good\" + 0.011*\"fried\" + 0.011*\"pork\" + 0.011*\"cheese\" + 0.010*\"like\" '\n",
      "  '+ 0.010*\"sweet\"'),\n",
      " (17,\n",
      "  '0.049*\"thai\" + 0.035*\"food\" + 0.022*\"curry\" + 0.021*\"place\" + 0.014*\"great\" '\n",
      "  '+ 0.012*\"good\" + 0.009*\"service\" + 0.008*\"really\" + 0.008*\"restaurant\" + '\n",
      "  '0.008*\"toronto\"'),\n",
      " (18,\n",
      "  '0.021*\"great\" + 0.017*\"place\" + 0.014*\"good\" + 0.013*\"beer\" + 0.013*\"food\" '\n",
      "  '+ 0.012*\"nice\" + 0.011*\"really\" + 0.010*\"menu\" + 0.009*\"drinks\" + '\n",
      "  '0.008*\"night\"'),\n",
      " (19,\n",
      "  '0.018*\"minutes\" + 0.017*\"order\" + 0.017*\"table\" + 0.013*\"time\" + '\n",
      "  '0.012*\"service\" + 0.010*\"back\" + 0.010*\"wait\" + 0.010*\"ordered\" + '\n",
      "  '0.010*\"told\" + 0.009*\"asked\"')]\n",
      "3.645236333211263\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "start = time()\n",
    "lda_model = gensim.models.LdaMulticore(workers=3,corpus=corpus,id2word=id2word, num_topics=20, random_state=100,passes=5)\n",
    "end = time()\n",
    "# Print the Keyword in the 15 topics\n",
    "pprint(lda_model.print_topics())\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lda_model[corpus[0]]\n",
    "result =sorted(result,key=lambda x:(-x[1],x[0])) \n",
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list = lda_model.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list = sorted(prob_list,key=lambda x:(-x[1],x[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(527, 0.024300857),\n",
       " (1290, 0.023184247),\n",
       " (862, 0.016006215),\n",
       " (171, 0.015837625),\n",
       " (139, 0.015623395)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 3\n",
    "val1 = 4\n",
    "val2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(val == 4 or val2 == 5 or va3 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
